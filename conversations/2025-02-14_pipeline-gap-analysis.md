# 当前流程的缺陷与缺失分析

> **日期**：2025-02-14
> **背景**：在完成 4 阶段流水线（元素提取 → 圣经编写 → 分镜脚本 → 视频生成提示词）初版后，对照好莱坞制作规范和当前 AI 能力（Seedream / Seedance 2.0）进行的全面 review。
> **结论**：原 4 阶段扩展为 7 阶段，新增视觉资产生成、首帧合成、剪辑后期三个关键阶段。

---

## 用户提出的分析框架

判断缺失的依据：

1. **目标**：依赖 AI 来自动化将 novel 制作成世界级别的电影。请结合业界电影制作的规范流程，尤其是大型世界级别的电影制作流程，可参考好莱坞。
2. **限制**：请结合最新业界的 AI 能力，尤其是结合字节跳动推出的 Seedream（AI 图片生成）、Seedance 2.0（AI 视频生成）。
   - Seedance 2.0 视频生成有个限制，一次最多能生成 15s 的视频（正常情况是 8-10s 左右）。
   - 为了突破该限制，目前常见做法：
     1. 先将角色、场景等生成图片，然后将小说做成分镜头，然后根据分镜头让 AI 去匹配角色、场景等生成图片，生成分镜头的首帧图，最后用视频生成 prompt 加首帧图生成视频，然后用剪辑工具把分镜头拼接起来。
     2. 先将角色、场景等生成图片，然后将小说做成分镜头，然后根据分镜头让 AI 去匹配角色、场景等生成图片。最后用视频生成 prompt 加匹配到的图片作为参考，生成视频，然后用剪辑工具把分镜头拼接起来。

---

## 分析结果

### 缺失1：视觉资产生成（最关键的缺失）

当前流程在阶段2（圣经编写）和阶段3（分镜）之间存在一个**断裂**。

圣经中写了 AI 绘图提示词，但**谁来执行这些提示词、生成图片、筛选确认、建立视觉基线？** 当前流程没有回答这个问题。而这恰恰是 Seedream + Seedance 工作流的核心前提——Seedance 2.0 的 image-to-video 模式需要高质量的参考图/首帧图作为输入，没有这些图片，视频生成无从谈起。

需要一个**视觉资产生成与确认**阶段：用 Seedream 跑圣经中的提示词 → 生成角色肖像、场景概念图、道具图 → 人工筛选确认 → 形成视觉资产库（image assets），后续分镜和视频生成都引用这个资产库。

### 缺失2：整体视觉风格定义（Art Direction）

好莱坞制作中，在画任何概念图之前，都有一个 **Look Development / Art Direction** 阶段——定义整部电影的视觉风格基调：写实还是风格化？色彩体系？光影风格？时代质感？

当前流程中，每个角色圣经各自定义颜色基调，但没有一个**全局的视觉风格文件**统领所有元素。这会导致：角色A的风格和场景B的风格可能不统一。Seedream 生成图片时，如果没有统一的 style prompt 前缀，各个元素之间会"画风不一致"。

### 缺失3：分镜阶段跳跃太大

当前"阶段3：分镜脚本"试图从小说直接跳到分镜头，这在好莱坞流程中是不存在的。实际流程是：

**小说 → 剧本改编（Screenplay）→ 场次分解（Scene Breakdown）→ 镜头表（Shot List）→ 分镜头（Storyboard）**

对于 AI 制作流，剧本改编可能可以简化，但**场次分解**是不可跳过的——它决定了"这部小说被分成多少个场景，每个场景的时长、地点、出场角色是什么"。这直接影响后续每个 8-15s 视频片段如何切分。

### 缺失4：首帧生成（Keyframe Composition）

这是工作流中最关键的一环，但当前流程完全没有覆盖。

Seedance 2.0 的最佳实践是 **First/Last Frame 模式**——上传 1-2 张图片作为起止帧，再加文字提示词生成视频。这意味着在"视频生成提示词"之前，还需要一个**首帧合成**阶段：根据分镜头的画面描述，从视觉资产库中调取对应的角色图、场景图，合成每个镜头的首帧图。

### 缺失5：视频片段拼接与后期

当前流程止于"视频生成提示词"，但生成出来的是一堆 8-15s 的片段。还需要：镜头衔接/转场设计、节奏控制、音频（Seedance 2.0 可同步生成音频，但对白、配乐、音效可能仍需额外处理）、整体剪辑。

### 缺失6：一致性验证机制

角色一致性是 AI 视频制作的头号难题。当前流程依赖圣经中的文字描述来保证一致性，但没有一个**显式的验证环节**——在生成了 100 个镜头后，谁来检查角色在第 1 个镜头和第 100 个镜头中长得一样？

Seedance 2.0 支持 Full Reference 模式（用参考图约束生成），Seedream 4.0 支持角色一致性融合，但这些能力需要在流程中被**显式调用和管控**。

### 现有阶段的问题

**阶段4 定义过窄**——当前叫"视频生成提示词"，但实际上 Seedance 2.0 的输入不只是文字 prompt，而是**文字 + 参考图 + 首帧图 + 音频**的多模态输入。仅生成文字 prompt 是不够的。

---

## 修正后的 7 阶段流程

```
原始小说
    │
    ▼
阶段1：元素提取        → 元素清单
    │
    ▼
阶段2：圣经编写        → 角色/场景/道具圣经（含AI提示词）
    │
    ▼
阶段3：视觉资产生成     → 角色肖像、场景概念图、道具图（NEW）
    │         ↑
    │    （一致性验证循环）
    │
    ▼
阶段4：场次分解与分镜    → 场次表 + 镜头表 + 分镜脚本（拆分原阶段3）
    │
    ▼
阶段5：首帧合成        → 每个镜头的首帧图（NEW）
    │
    ▼
阶段6：视频生成        → 多模态输入包（prompt + 首帧 + 参考图）→ 8-15s片段（扩展原阶段4）
    │
    ▼
阶段7：剪辑与后期      → 转场、音频、节奏、最终输出（NEW）
```

加了三个新阶段（3、5、7），拆分了一个（原3→新4），扩展了一个（原4→新6）。

---

## 后续行动

分析通过后，已将 `skills/novel-to-film/SKILL.md` 按此 7 阶段结构完整重写。
